{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from itertools import combinations, combinations_with_replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Using gpu: %s ' % torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Thinking like Transformers](https://arxiv.org/abs/2106.06981)\n",
    "\n",
    "Here we code our 'toy' GPT without any training in order to compute histograms.\n",
    "\n",
    "First start by coding your Self-Attention layer (do not forget to choose properly your initialization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.in_channels = config.in_channels\n",
    "        self.out_channels = config.out_channels\n",
    "        self.key_channels = config.key_channels\n",
    "        self.Query = nn.Linear(self.in_channels, self.key_channels)\n",
    "        self.Key = nn.Linear(self.in_channels, self.key_channels)\n",
    "        self.Value = nn.Linear(self.in_channels, self.out_channels)\n",
    "    \n",
    "    def _init_hist(self):\n",
    "        #\n",
    "        # your code here\n",
    "        #\n",
    "          \n",
    "        \n",
    "    def forward(self, x): # x (bs, T, ic)\n",
    "        Q = self.Query(x) # (bs, T, kc)\n",
    "        K = self.Key(x)/math.sqrt(self.key_channels) # (bs, T, kc)\n",
    "        V = self.Value(x) # (bs, T, oc)\n",
    "        A = #\n",
    "        # your code here\n",
    "        #\n",
    "        return y, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class toy_config:\n",
    "    in_channels = 3\n",
    "    out_channels = 3\n",
    "    key_channels = 3\n",
    "    \n",
    "sa_toy = SelfAttentionLayer(toy_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(5,10,3)\n",
    "y,A = sa_toy(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now code your 'toy' transformer block and your 'toy' GPT to compute histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block_hist(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attn = SelfAttentionLayer(config)\n",
    "        self.final_function = # your code here\n",
    "        self.attn._init_hist()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x,_ = self.attn(x)\n",
    "        x = self.final_function(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT_hist(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.in_channels = config.in_channels\n",
    "        self.tok_emb = nn.Embedding(self.in_channels,self.in_channels)\n",
    "        self.block = Block_hist(config)\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        #\n",
    "        # your code here\n",
    "        #\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        x = self.tok_emb(idx)\n",
    "        x = self.block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your implementation by first choosing properly your configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_digits = 4\n",
    "class config:\n",
    "    in_channels=nb_digits+1\n",
    "    out_channels=1\n",
    "    key_channels=nb_digits+1\n",
    "    #max_hist = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh = GPT_hist(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh(torch.tensor([0,1,1,2,3,2,1]).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating your dataset\n",
    "\n",
    "Now, we will use a 'micro' GPT to learn the task of histograms. We will use your 'toy' GPT to generate the dataset. Since GPT is equivariant, we can indeed compute all possible different outputs and this number is not too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_train = 30\n",
    "nb_digits = 4\n",
    "comb = combinations_with_replacement(range(0,seq_train+1), nb_digits-1)\n",
    "\n",
    "def make_seq(c, seq_train):\n",
    "    c_l = [0] + list(c) + [seq_train]\n",
    "    len_seq = len(c_l)-1\n",
    "    return [c_l[i+1]-c_l[i] for i in range(len_seq)]\n",
    "\n",
    "l_comb =  [make_seq(c,seq_train) for c in comb]\n",
    "\n",
    "len(l_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.comb(seq_train+nb_digits-1, nb_digits-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inputs(l_comb, nb_digits=nb_digits):\n",
    "    inputs = []\n",
    "    for t in l_comb:\n",
    "        curr = [0]\n",
    "        for (i,j) in enumerate(t):\n",
    "            curr += [i+1 for _ in range(j)]\n",
    "        inputs.append(torch.tensor(np.array(curr)))\n",
    "    return inputs\n",
    "\n",
    "def duplicate(l, n):\n",
    "    return [e for e in l for _ in range(n)]\n",
    "\n",
    "def make_loader(len_seq,nb_digits,size):\n",
    "    comb = combinations_with_replacement(range(0,len_seq+1), nb_digits-1)\n",
    "    l_comb =  [make_seq(c,len_seq) for c in comb]\n",
    "    inputs = make_inputs(l_comb)\n",
    "    labels = [(gh(d.unsqueeze(0)).squeeze(0).squeeze(1)).type(torch.LongTensor) for d in inputs]\n",
    "    dataset = list(zip(inputs,labels))\n",
    "    n_dup = size // math.comb(len_seq+2, nb_digits-1)\n",
    "    dataset_l = duplicate(dataset,n_dup)\n",
    "    len_in = len(dataset_l)\n",
    "    loader = torch.utils.data.DataLoader(dataset_l, batch_size=128, shuffle=True)\n",
    "    return loader, len_in, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, size_train, inputs_train = make_loader(seq_train,nb_digits,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_in = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding 'micro' GPT\n",
    "\n",
    "Now we need to code the 'micro' GPT used for learning. The game here is to reuse your `SelfAttentionLayer` above without any modification. The only part that is modified is the hard-coded `final_function` which now replaced by a MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attn = SelfAttentionLayer(config)\n",
    "        self.mlp = # your code here\n",
    "\n",
    "    def forward(self, x): # x (bs, T,ic)\n",
    "        #\n",
    "        # your code here\n",
    "        #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.in_channels = config.in_channels\n",
    "        self.nb_digits = config.nb_digits\n",
    "        #\n",
    "        # your code here\n",
    "        #\n",
    "        \n",
    "    def forward(self, idx, targets=None, verbose=False):\n",
    "        # shape of idx: (bs, len) 0=bos and 1...nb_digits\n",
    "        # shape of targets: (bs, len)\n",
    "        #\n",
    "        # your code here\n",
    "        #\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = # your code here\n",
    "        if verbose:\n",
    "            return logits, loss, A\n",
    "        else:\n",
    "            return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config_gpt:\n",
    "    nb_digits = nb_digits\n",
    "    in_channels = 12 \n",
    "    out_channels = in_channels \n",
    "    key_channels = 128 \n",
    "    max_hist = seq_train+1\n",
    "    \n",
    "gptmini = GPT(config_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, _ = gptmini(batch_in[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,preds = torch.max(logits,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_in[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(preds == batch_in[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, size, epochs=1, optimizer=None):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        n_batch = 0\n",
    "        for inputs,targets in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            logits, loss = model(inputs,targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #\n",
    "            # complete the code below:\n",
    "            _,preds = torch.max(logits,-1)\n",
    "           \n",
    "            running_corrects += \n",
    "            running_loss += \n",
    "            n_batch += 1\n",
    "        epoch_loss = running_loss /n_batch\n",
    "        epoch_acc = running_corrects.data.item() /n_batch\n",
    "        print('Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                     epoch_loss, epoch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gptmini = GPT(config_gpt)\n",
    "gptmini = gptmini.to(device)\n",
    "lr = 0.01\n",
    "optimizer = torch.optim.Adam(gptmini.parameters(),lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = (seq_train+1)*size_train\n",
    "train_model(gptmini,train_loader,size_train,25,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(gptmini.parameters(),lr = lr)\n",
    "train_model(gptmini,train_loader,len_train,15,optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalization\n",
    "\n",
    "Adapt your code to be able to deal with smaller sequences (with possibly fewer digits). Hint: use padding...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dldiy",
   "language": "python",
   "name": "dldiy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
